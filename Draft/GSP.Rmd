---
title: "Good Statistical Practice (GSP): guidence for using R for scientific publication"
author:
- familyname: Gao
  othernames: Caroline X.
  address: Centre for Youth Mental Health, University of Melbourne; School of Public Health and Preventive Medicine, Monash University, Melbourne, Australia
  email: caroline.gao@orygen.org.au
- familyname: Hamilton
  othernames: Matthew
  address: Orygen, Melbourne, Australia
output:
   bookdown::pdf_document2:
     toc: yes
     number_sections: false
     latex_engine: xelatex
     pandoc_args:
      - --template=template.tex
date: "`r format(Sys.time(), '%B %d, %Y')`" 
geometry: margin=1in
fontsize: 11pt
bibliography: GSP.bib 
header-includes:  
    \usepackage{float} 
    \floatplacement{figure}{H} 
    \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}}   \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
    \usepackage{lscape}
    \newcommand{\blandscape}{\begin{landscape}}
    \newcommand{\elandscape}{\end{landscape}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
pkgs <- c("tidyverse", "emo")
```


# Preface

This short practice guidance is designed based on a few guidelines and practice principles, books and jounal articles, including: 

* [ASA "Ethical Guidelines for Statistical Practice"](https://www.amstat.org/ASA/Your-Career/Ethical-Guidelines-for-Statistical-Practice.aspx)
* [Reproducible Research with R and R Studio](https://englianhu.files.wordpress.com/2016/01/reproducible-research-with-r-and-studio-2nd-edition.pdf)
* [Efficient R programming](https://csgillespie.github.io/efficientR/)

The aim of this guidance is to promote accountability, reproducibility and integrity of statistical practice in Orygen health service and outcome research team.  The guidance is divided into four parts: 1) planning for analysis, 2) data cleaning, 3) analysis, and 4) reporting.  

Important note: **the authors do not give permission for you to print this resource on papers, unless you are experimenting magical ink that disappears after 3 months (you are considered to have net-zero carbon emissions in this case).**  If you are thinking about reading this on paper before going to sleep, you are too opportunistic for your study plan : P 

Have fun ~~ 

# Planning for anlaysis 

## Dafting the anlaysis plan

The analysis will need to be planed before you touch the data. Your analysis may deviate from the analysis plan to some degrees, which may relate to data integrity of the variable selected, model fitting factors (i.e. multicollinearity, heteroscedasticity etc) and change of research questions. However, unless your aim is purely exploratory (i.e. identify latent clusters) or predictive, your analysis should be guided by the analysis plan to prevent "fishing" results. Remember "If you torture the data long enough, it will confess to anything - Ronald H. Coase", which is really against the scientific principle. The scientific evidence is  based on a collection of studies and findings rather than a single paper. If you have a negative finding, you are obligated to publish it !!! 

## Less is more

Be aware of the "Complexity Bias".  "Life is really simple, but we insist on making it complicated ".
```{r , echo=FALSE,fig.align="center"}
knitr::include_graphics(here::here("Graphics/quote.jpeg"))
```


The famous quote is not by Confucius (it is from the New Book of Tang published about 1500 years after his death), and it is not well translated, but you get the idea. We often find it easier to face a complex problem than a simple one, and often feel that a complex solution must be better than an easier one. However this is often not true in applied statistics. 

More often than not, ingenious statistical designs are surprisingly simple. An famous example is the Cox model for survival analysis, which simplifies the needs to describe the baseline hazard function with proportional hazard assumption. Another example, Dijkstra's algorithm (algorithm for finding the shortest paths between nodes in a graph), the author, Edsger Dijkstra, once said "One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. " 

This is the same with your analysis, always force yourself to avoid complexities if you could achieve what you need with a simpler model. There are numerous benefits for simpler models, i.e. less likely to over-fitting with your model, easier to communicate with your audience, less likely to make mistakes etc. If a logistic regression works, there is no need to use a structure equation model. 

## Officially certified plans

Get your analysis plan approved or reviewed. I think we all do this to some degrees. Some studies have strict protocols on who and when the analysis plan will need to be approved together with other ethical requirements. Other studies will only require you to discuss with your supervisors. Regardless, it will be better to have a written analysis plan with review and/or approval and avoids confusions down the track. Sometimes you might want or need to [preregister](https://www.apa.org/science/about/psa/2015/08/pre-registration) your study, which is considered as a part of the open science practice. 


# Use R

The essential skill set in the modern analytical community is the ability to use one or more script languages, SPSS doesn't count here. It's often critical for the success of your publication. More often then not, you might face challenges from the reviewer to do something that SPSS is not capable of. If you have already started your analysis in SPSS, please abandoned it ASAP. If R is in its early adolescent years, SPSS is a toddler and it suffers from the Peter Pan Syndrome (it will never grow up). 

```{r pressure, echo=FALSE, out.width = '100%'}
knitr::include_graphics(here::here("Graphics/R_illustration.jpg"))
```


Sorry for being a bit offensive, but the reality is the industry and scientific community are gradually moving away from SPSS to other languages for many reasons, cost, capacity, flexibility, reporducibility etc. So to avoid the long term pain, change to R (Stata is also good, since this is a practice guide for R, I won't touch too much on Stata). 


# Learn to use R

"R being hard to learn" mostly exists in our fears. Once you get on to it the challenges are mostly "feeling a bit confused", there are so many packages, so many ways to do the same thing, so much typing needed, so many materials everywhere... So the best way to learn R is to learn by using it. You can start with a short intensive introduction course (one or two days) to build up your confidence. Start using it in a small project, and then gradually roll out to more parts of your analysis tasks. You can also start by learning your collaborator's code (communicate with the author and create a separate environment so that you won't break anything).

There are lots of good online training materials for R:

* [R for Reproducible Scientific Analysis from Software Carpentry ](https://swcarpentry.github.io/r-novice-gapminder/)
* [R Programming on Coursera by Roger Peng](https://www.coursera.org/learn/r-programming),
* [An Introduction to R](https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf)
* [R for Data Science](https://r4ds.had.co.nz/). 

If you are a skilled R user but do not use Rmarkdown or Bookdown, I would also recommend you to read Yihui Xie's books: 

* [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/)
* [bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/) (for advanced users)

Almost all R users are still learning R!! So you have no excuse not to ~~ 

# R setup, project managment and Rmarkdown

## Setup R on your computer

One thing that you have to remember: R is a fast evolving language. It has a new version every few months (sometimes two versions in one months), with funny names : ) 
```{r}
library(rversions)
tail(r_versions())
```
Although R kept on updating, you do not need to re-install R all the time. But I tend to update my R half-yearly. It doesn't take a long time to update everything now a days. The first thing that you need to do after installing R is to install your commonly used packages. A good way to install + load packages is to use the [*pacman*](https://cran.r-project.org/web/packages/pacman/index.html) package, which is much faster than typing install.package() and library(). I normally store the names of my commonly used packages somewhere. So when I need to re-install R, I will call *pacman* to install all of those packages for me at once (only takes about 10 minutes). 

```{r}
#load libraries 
library(pacman)
p_load("dplyr", "tidyr", "ggplot2") 
```


*install.package()* and *pacman* install packages from R CRAN. However, the authors may have additional update on Github which is not loaded to CRAN. You can install the most resent package using *devtools*

```{r, eval=F}
library(devtools)
install_github('rstudio/bookdown')
```

To get start with things, just install it from R CRAN. If you came across with some problems which seems to be a software "bug" you can update your package using *install_github*. You can also check whether your issue was being reported on Github (i.e. https://github.com/rstudio/bookdown/issues).

## Keep tracking of R and package versions

For all your analysis, you need to keep track of the R environment, this includes R version, platform and all package versions that were loaded in your current analysis environment. You can easily get a summary using *sessionInfo*. 
```{r}
sessionInfo()
```
This is crucial as  packages or R updates may make your code fail or your results different. This sounds scary than it really is. All analysis packages will have this issue. It's slightly complected with R because updates of R and affiliated packages are not synchronized. For my 13 years of using R, I haven't found this particularly challenging for a few reasons. The packages update rates were not as fast as we are seeing today. Most of my projects were in isolated environment so I rarely need to re-run the analysis a few years later and will need to produce exactly the same results. If I need to re-run the analysis, I will mostly update the analysis code and make sure it adhere to the current best practice. Occasionally, some of my package updates will cause problems with my current analysis code. As I keep track of my R environment, I will be able to figure out the problem quickly and re-install the older version back. 

```{r, eval=F}
require(devtools)
install_version("bookdown", version = "0.20", 
                repos = "http://cran.us.r-project.org")
```

If you are in the area that requires 100% reproducibility all the time, you can use [**Microsoft R open**](https://mran.microsoft.com/open) together with *checkpoint* which lock down the set of packages in your project environment, so package updates will not impact your code. There are a few other alternatives, but you can get other benefits from **Microsoft R open** without too much trouble, so it seems to be a wise choice at the current stage. If you need to install lots of things from Github and work on package development, it may not be a good choice. 

## Project managment with R

Over the years of being a programmer and biostatistician, I have learned many things in a painful way. One of them is the importance of good project management with your data, analysis, documentation and drafts. I had to spent a long time to remember what I did, where I save things and also lots of detective work on whether anyone have touched my 'cheese' (with files stored on network drives). When I am lucky I can request IT to restore a backup at specific time to find the lost treasure. But many cases, this doesn't work as either I was stupid enough not to save things on the network drive, or the network drive deletes backups automatically. 

Project management became more critical when I had to change my work between computers, laptops and servers as well as swap frequently between Windows and Mac. Managing file versions, backups and file paths became additional burdens. However, all of these things became much more easier with RStudio + Github. It's also more user friendly with people who are not familiar with Git.  
Essentially the idea is that you can create an independent project environment for isolated task or tasks that shares common data source. This can be a specific analysis for a paper or a range of analysis base on the same dataset. When there are multiple users, it's always the best to use separate project environment for individual analysis, so users do not load the same project file on the network drive.

The method for setting up projects are described [here](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects). There are many benefits using projects:

* No need to type the long path directory in the project environment because it is using relative file path. 
* You can use [here](https://github.com/jennybc/here_here) package when working with Rmarkdown files within the project environment. The default file path in Rmarkdown is the Rmarkdown file location, using here package, it always point to the project directory. So you have your path directory consistent when using console and Knit (see Rmarkdown in the next session).
* Access all files in the project environment easily 
* Easy to communicate with Github

You can setup a rule for organising sub-folders and files. I like to separate code, data and results. However, it is slightly complicated to split analysis code and results when you are using Rmarkdown. Regardless, it would be better to have sub-folder structure in placed so you can find files easily.

## Github 


https://swcarpentry.github.io/git-novice/




## State-of-art Rmarkdown/Rnotebook paractice


# Data cleaning 

## Import data 

## Tidy-version data cleaning routine 

## Notes for yourself and others

## Validity checking 

## Common pitfalls 

## Documentation for the never ending data cleaning process 

# Analysis 

## Exploratory phase 

## Statistical modeling with R 

## Extract results

## Advanced topics

### Loops

### Functions 

### Render anlaysis results from different dataset with the same rmarkdown file


# Reporting 

## One-stop shop 

## A good graph takes forever  

## Write up of the analysis results

* Report the nature and source of the data, validity of instrument and data collection process ( i.e. response rate and any possible bias).

* Report any data editing procedures, including any imputation and missing data mechanisms 

* When reporting analyses of volunteer data or other data that may not be representative of a defined population, includes appropriate disclaimers and, if used, appropriate weighting.

* Include the complete picture of the analysis results, which may require presenting tables and figures in appendix tables. For example, when reporting a series of multivariate regression models between an exposure and different outcomes, you can choose to include a summary table of adjust coef between exposure and different outcomes in the main text and include all the individual regression model results in the Appendix. The reader can use the appendix tables to understand the impact of confounding variables in the model.

* Report prevalence of outcomes or weighted prevalence of outcomes for representative samples. 

* Report point estimate, 95% confidence interval and p-value in results

* Use graphical representations for reporting interaction effects (marginal plot)

* Acknowledges statistical and substantive assumptions made in the execution and interpretation of any analysis. 

* Reports the limitations of statistical inference and possible sources of error.

* Where appropriate, addresses potential confounding variables not included in the study.

* Conveys the findings in ways that are meaningful and visually apparent and to the user/reader. This includes properly formatted tables and meaningful graphics (use guidelines by @Gordon_2015).

* To aid peer review and replication, shares the data (or synthetically generated data) used in the analyses whenever possible/allowable

* Provide all analysis code either as an Appendix or in open repositories such as Github

## Advanced topics

### Write a paper using R

### Advanced Latex 

# Version control 

## Version control framework

## Github 

# Publication 

# Reference
