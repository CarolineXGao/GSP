---
title: "Good Statistical Practice (GSP): guidence for using R for scientific publication"
author:
- familyname: Gao
  othernames: Caroline X.
  address: Centre for Youth Mental Health, University of Melbourne; School of Public Health and Preventive Medicine, Monash University, Melbourne, Australia
  email: caroline.gao@orygen.org.au
- familyname: Hamilton
  othernames: Matthew
  address: Orygen, Melbourne, Australia
output:
   bookdown::pdf_document2:
     toc: yes
     number_sections: false
     latex_engine: xelatex
     pandoc_args:
      - --template=template.tex
date: "`r format(Sys.time(), '%B %d, %Y')`" 
geometry: margin=1in
fontsize: 11pt
bibliography: GSP.bib 
header-includes:  
    \usepackage{float} 
    \floatplacement{figure}{H} 
    \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}}   \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
    \usepackage{lscape}
    \newcommand{\blandscape}{\begin{landscape}}
    \newcommand{\elandscape}{\end{landscape}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
pkgs <- c("dplyr", "tidyr","tidyverse", "emo")
```


# Preface

This short practice guidance is designed based on a few guidelines and practice principles, books and jounal articles, including: 

* [ASA "Ethical Guidelines for Statistical Practice"](https://www.amstat.org/ASA/Your-Career/Ethical-Guidelines-for-Statistical-Practice.aspx)
* [Reproducible Research with R and R Studio](https://englianhu.files.wordpress.com/2016/01/reproducible-research-with-r-and-studio-2nd-edition.pdf)
* [Efficient R programming](https://csgillespie.github.io/efficientR/)

The aim of this guidance is to promote accountability, reproducibility and integrity of statistical practice in Orygen health service and outcome research team.  The guidance is divided into four parts: 1) planning for analysis, 2) data cleaning, 3) analysis, and 4) reporting.  

Important note: **the authors do not give permission for you to print this resource on papers, unless you are experimenting magical ink that disappears after 3 months (you are considered to have net-zero carbon emissions in this case).**  If you are thinking about reading this on paper before going to sleep, you are too opportunistic for your study plan : P 

Have fun ~~ 

# Planning for anlaysis 

Rule number 1: The analysis will need to be planed before you touch the data. Your analysis may deviate from the analysis plan to some degrees, which may relate to data integrity of the variable selected, model fitting factors (i.e. multicollinearity, heteroscedasticity etc) and change of research questions. However, unless your aim is purely exploratory (i.e. identify latent clusters) or predictive, your analysis should be guided by the analysis plan to prevent "fishing" results. Remember "If you torture the data long enough, it will confess to anything - Ronald H. Coase", which is really against the scientific principle. The scientific evidence is  based on a collection of studies and findings rather than a single paper. If you have a negative finding, you are obligated to publish it !!! 

Rule number 2: Be aware of the "Complexity Bias".  "Life is really simple, but we insist on making it complicated ".
```{r , echo=FALSE,fig.align="center"}
knitr::include_graphics(here::here("Graphics/quote.jpeg"))
```


The famous quote is not by Confucius (it is from the New Book of Tang published about 1500 years after his death), and it is not well translated, but you get the idea. We often find it easier to face a complex problem than a simple one, and often feel that a complex solution must be better than an easier one. However this is often not true in applied statistics. 

More often than not, ingenious statistical designs are surprisingly simple. An famous example is the Cox model for survival analysis, which simplifies the needs to describe the baseline hazard function with proportional hazard assumption. Another example, Dijkstra's algorithm (algorithm for finding the shortest paths between nodes in a graph), the author, Edsger Dijkstra (the author), once said "One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. " 

This is the same with your analysis, always force yourself to avoid complexities if you could achieve what you need with a simpler model. There are numerous benefits for simpler models, i.e. less likely to over-fitting with your model, easier to communicate with your audience, less likely to make mistakes etc. If a logistic regression works, there is no need to use a structure equation model. 

Rule number 3: Get your analysis plan approved or reviewed. I think we all do this to some degrees. Some studies have strict protocols on who and when the analysis plan will need to be approved together with other ethical requirements. Other studies will only require you to discuss with your supervisors. Regardless, it will be better to have a written analysis plan with review and/or approval and avoids confusions down the track. Sometimes you might want or need to [preregister](https://www.apa.org/science/about/psa/2015/08/pre-registration) your study, which is considered as a part of the open science practice. 

Last rule: Choose a script language (by no means SPSS) for your analysis. If you have already started your analysis in SPSS, please abandoned it ASAP. My metaphor: If R is in its early adolescent years, SPSS is a toddler and it suffers from the Peter Pan Syndrome (it will never grow up). 

```{r pressure, echo=FALSE, out.width = '100%'}
knitr::include_graphics(here::here("Graphics/R_illustration.jpg"))
```


Sorry for being a bit offensive, but the reality is that most of the skilled SPSS users that I know have already or have been considering to learn R or Stata. The rest of them are no longer doing much analysis. So to avoid long term pain, change to R (Stata is also good, since this is a practice guide for R, I won't touch too much on Stata). There are lots of good online training materials for R:

* [R for Reproducible Scientific Analysis from Software Carpentry ](https://swcarpentry.github.io/r-novice-gapminder/)
* [R Programming on Coursera by Roger Peng](https://www.coursera.org/learn/r-programming),
* [An Introduction to R](https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf)
* [R for Data Science](https://r4ds.had.co.nz/). 

If you are a skilled R user but do not use Rmarkdown or Bookdown, I would also recommend you to read Yihui Xie's two books: 

* [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/)
* [bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/) (for advanced users)

Well, all books from bookdown.org are worth reading. 

One more remark: you can use causal diagrams to assist the design your analysis. 

# R setup, project managment and Rmarkdown

## Setup R on your computer

One thing that you have to remember: R is a fast evolving language. It has a new version every few months (sometimes two versions in one months) with funny names : ) 
```{r}
library(rversions)
tail(r_versions())
```
Although R kept on updating, you do not need to re-install R all the time. But I tend to update my R half-yearly. It doesn't take a long time to update everything now a days. The first thing that you need to do after installing R is to install your commonly used packages. A good way to install + load packages is to use the [*pacman*](https://cran.r-project.org/web/packages/pacman/index.html) package, which is much faster than typing install.package() and library(). I normally store the names of my commonly used packages somewhere. So when I need to re-install R, I will call *pacman* to install all of those packages for me at once (only takes about 10 minutes). 

```{r}
#load libraries 
library(pacman)
p_load("dplyr", "tidyr", "ggplot2") 

```

## Project managment with R

## Use Rmarkdown/Rnotebook for everything

# Data cleaning 

## Import data 

## Tidy-version data cleaning routine 

## Notes for yourself and others

## Validity checking 

## Common pitfalls 

## Documentation for the never ending data cleaning process 

# Analysis 

## Exploratory phase 

## Statistical modeling with R 

## Extract results

## Advanced topics

### Loops

### Functions 

### Render anlaysis results from different dataset with the same rmarkdown file


# Reporting 

## One-stop shop 

## A good graph takes forever  

## Write up of the analysis results

* Report the nature and source of the data, validity of instrument and data collection process ( i.e. response rate and any possible bias).

* Report any data editing procedures, including any imputation and missing data mechanisms 

* When reporting analyses of volunteer data or other data that may not be representative of a defined population, includes appropriate disclaimers and, if used, appropriate weighting.

* Include the complete picture of the analysis results, which may require presenting tables and figures in appendix tables. For example, when reporting a series of multivariate regression models between an exposure and different outcomes, you can choose to include a summary table of adjust coef between exposure and different outcomes in the main text and include all the individual regression model results in the Appendix. The reader can use the appendix tables to understand the impact of confounding variables in the model.

* Report prevalence of outcomes or weighted prevalence of outcomes for representative samples. 

* Report point estimate, 95% confidence interval and p-value in results

* Use graphical representations for reporting interaction effects (marginal plot)

* Acknowledges statistical and substantive assumptions made in the execution and interpretation of any analysis. 

* Reports the limitations of statistical inference and possible sources of error.

* Where appropriate, addresses potential confounding variables not included in the study.

* Conveys the findings in ways that are meaningful and visually apparent and to the user/reader. This includes properly formatted tables and meaningful graphics (use guidelines by @Gordon_2015).

* To aid peer review and replication, shares the data (or synthetically generated data) used in the analyses whenever possible/allowable

* Provide all analysis code either as an Appendix or in open repositories such as Github

## Advanced topics

### Write a paper using R

### Advanced Latex 

# Version control 

## Version control framework

## Github 

# Publication 

# Reference
